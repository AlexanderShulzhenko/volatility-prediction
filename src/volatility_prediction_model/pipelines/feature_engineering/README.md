# Feature engineering
![features](https://github.com/AlexanderShulzhenko/volatility-prediction/assets/80621503/cdc784b6-2404-4176-ba0e-9e12e0db32f4)

## Intro
This pipeline generated features from the primary layer tables. Feature engineering pipeline leverages `numba` for just-in-time compilation of certain function to improve the pipeline performance.

## Candlestick features
This section contains basic features that could be derived using OHLC data. 

## Indicators features
This section implements certain techincal indicators that are widely used, e.g., smoothed RSI, Bollinger bands and Stochactic oscilator. Interesting part of this section is the slope calculation for the last `n` time points (i.e., rolling linear regression), which in performed by the:
```python
@jit(nopython=True) 
def line_fit(
  target_values: np.array,
  window_size: int
  ) -> Tuple[List[float], List[float]]:
  # calculation body
  return coefs, r2s
```
This function is calculating slopes using OLS estimator and $R^2$ metrics for each fit.

## Trades featureson
This section performs in-depth analysis of the trades data. Base statistics as skewness and kurtosis are calculated, on other side statistical tests are performed to check the distributions fit for popular distributions, e.g., $\mathcal{Norm(0, 1)}$ or $\mathcal{Cauchy(0, 1)}$. During this step, each trades data frame is validated using `pandera.check_output()` function. The schema that was used to validate the trades data is:
```python
DataFrameSchema(
    {
        "Price": Column("float32", required=True),
        "Quantity": Column("float32", required=True),
        "Type": Column(str, required=True),
    }
)
```

## Stochastic features
This section introduces stochastic features for time series analysis. These features utilize theory of Gaussian processes to create a framework that evaluates the likelihood of time window being generated by the Gaussian process with one of the selected kernels. The reason behind using these features is that we are interested in expecting the mean-reverting property of the time series locally, i.e., during the certain time window. Kernels that are currently implemented are:
- Mat√©rn kernel: $K_\text{Matern}(x,x') = \tfrac{2^{1-\nu}}{\Gamma(\nu)} \left(\tfrac{\sqrt{2\nu}|d|}{\ell} \right)^\nu K_\nu \left(\tfrac{\sqrt{2\nu}|d|}{\ell} \right)$;
- Ornstein-Uhlenbeck kernel: $K_\text{OU}(x,x') = \exp \left(-\tfrac{|d|} \ell \right)$;

To test the hypothesis of the time series being generated by each of mentioned kernels we utilize certain properties of Gaussian processes:

For a given set of parameters $\beta = \{\kappa, \dots\}$ and sample $\textbf{X} = \{x_0, \dots, x_n\}$ we want to test the hypothesis that $$\textbf {X} \sim \mathcal{N}(0, K_{\beta}(\textbf{X}, \textbf{X});$$
This is equivalent to testing a hypothesis:
$$ \textbf{Y} = K_{\beta}(\textbf{X}, \textbf{X})^{\frac{1}{2}} \textbf{X} \sim \mathcal{N}(0, \mathbb{I}); $$
This problem is reduced to testing the hypothesis that
$$ \textbf{Y}_1, \dots, \textbf{Y}_n \overset{iid}{\sim} \mathcal{N}(0, 1)$$
which is the standard task of testing the hypothesis of sample normality. There are a large number of tests to test such a hypothesis; One of the most popular tests is the Shapiro-Wilk test \cite{shapiro}.
\item Optimize the parameters vector $\beta$ by minimizing the p-value from the previous step.
